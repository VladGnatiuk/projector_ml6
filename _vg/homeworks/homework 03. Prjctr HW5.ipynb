{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXB4rkrC_Nk4"
      },
      "source": [
        "# MLb6: Evaluating models. Validation. Metrics\n",
        "\n",
        "#### Describe validation approaches and the best metrics for different datasets:\n",
        "1. https://www.kaggle.com/c/titanic\n",
        "2. https://www.kaggle.com/c/petfinder-pawpularity-score/data\n",
        "3. https://www.kaggle.com/c/web-traffic-time-series-forecasting\n",
        "4. https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
        "\n",
        "\n",
        "#### For validation description:\n",
        "- For time series, use timestamps for train and validation periods. for example, train: 01.01.2012-01.01.2013, val: 01.01.2013-01.01.2014, 01.01.2013-01.01.2014, val: 01.01.2014-01.01.2015, etc)\n",
        "- Explain why do you choose hold-out or k-fold (mention k)\n",
        "\n",
        "\n",
        "#### For metrics description:\n",
        "- Explain why you choose one metric and not the analog. For example, I choose accuracy instead of f1-score because... or RMSE instead of MAE because.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Titanic\n",
        "- The data is limited and we can't get more of it (staying humanistic). To take maximum from every bit of information we need to use **K-fold Cross-validation** with high number k. From histogram charts above each column on kaggle page, it seems like shape of value distribution in train data and test data is about the same. Age column is a bit different and ticket column is hard to estimate without further analysis, but let's make no adjustments just yet. Making split randomly should make validation data look similar to test, so no extra steps for preparing vaidation data is needed.\n",
        "- If we had a complete dataset (train+test=2224 people) we could have used ROC AUC - knowing number of people that have to go into each category we would take x most likely survived people. Sice we don't know the exact number, we can't use ROC AUC. Accuracy won't work well because of calsses inbalance (only 1 of 4 survived).\n",
        "- I would pick F1 metric making precision and recall equally important.\n"
      ],
      "metadata": {
        "id": "cyUPzwjNJASy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pawpularity\n",
        "- For regression metric I would use **RMSE**: we don't want to be very much wrong on highly popular or higly unpopular cases. From comercial point of view those are probably the most important cases. And the dataset has a fat tail at 100. Rooted MSE is to have a more human-interpretable metric.\n",
        "- Large dataset, **Holdout or K-fold with low number k** should be enough for validation. The latter is better to make sure popular and unpopular cases got into valdation set."
      ],
      "metadata": {
        "id": "VK0IaMOMb1WP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Wikipedia\n",
        "- Number of month isn't big, use **K-fold Cross-validation** with maximum k that satisfies the size of train and size of test/validation.\n",
        "- For phase one test data is from 2017-01-01 to 2017-03-01 - 2 month and train data is from 2015-07-01 to 201-12-31 - 18 month. We want to make validation set also 2 month long to match test and train set at least 12 month long to capture complete year in case of seasonality of data, and we also want to maximize number k, so let's use the following folds:\n",
        "  - train: 2015-07-01 - 2016-06-30; val: 2016-07-01 - 2016-08-31\n",
        "  - train: 2015-08-01 - 2016-07-31; val: 2016-08-01 - 2016-09-30\n",
        "  - train: 2015-09-01 - 2016-08-31; val: 2016-09-01 - 2016-10-31\n",
        "  - train: 2015-10-01 - 2016-09-30; val: 2016-10-01 - 2016-11-30\n",
        "  - train: 2015-11-01 - 2016-10-31; val: 2016-11-01 - 2016-12-31\n",
        "- \"Submissions are evaluated on SMAPE between forecasts and actual values\" - **Symmetric mean absolute percentage error** is dictated to be used by the rules of the competition. I guess this metric was selected because of the following reasons:\n",
        "  - it's percentage base, so high page counts don't overshadow low counts\n",
        "  - it treats overestimates and underestimates equally\n",
        "  - counts are integer numbers, so we won't get too small numbers in the divisor calculating percentage.\n",
        "  - it has upper and low boundaries which serves interpretability.\n"
      ],
      "metadata": {
        "id": "LlWafbYsb2gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Toxic texts\n",
        "- The task is to create a multiheaded model to predict value in each toxicity label. We'll need to train 5 models - one for each column.\n",
        "- We have a lot of data, will use **Holdout with stratified sampling** to match % of toxic labels in validaton set with train set.\n",
        "- I assume that it's ok to false alarm a normal text as toxic because the cost of false positive is low: fewer people will see a post, a human review may take place. Use **F1 score** with coeficient beta (to be obtained in experiments) that **prioritizes recall** and satifies some minor number for precision metric.\n",
        "\n",
        "Hint from the teacher: to use ROC-AUC. As I understand the benefit of the method is in weighting the samples. We want to get rid of the most toxic commens first and may allow less defined samples to float around for a while. It also simplifies work of the moderator to have all samples ranked."
      ],
      "metadata": {
        "id": "CiZLjqudb-XE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvypQq9Qa3ZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}