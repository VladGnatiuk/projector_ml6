{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMms33J4jeDZ"
      },
      "source": [
        "1. Use the same dataset from the previous task\n",
        "2. Reuse validation strategy and preprocessing without changes\n",
        "3. Train xgboost model\n",
        "4. Train lightgbm model\n",
        "5. Train catboost model\n",
        "6. Compare performance on local validation and on test set on kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load train data\n",
        "# reuse the preprocessing approach from the previous homework\n",
        "\n",
        "# reuse validation approach from the previous homework. \n",
        "# it should be exactly the same because we want to compare the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the xgboost model (from xgboost package)\n",
        "# define the hyperparameters\n",
        "# train the model\n",
        "# try to improve the model by changing the hyperparameters on local validation (remember that using gridsearch is a bad idea, because it can't use the early stopping)\n",
        "# retrain the model on the whole train dataset\n",
        "# don't forget to specify the number of boosting rounds you found optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSixk8wXjZJZ"
      },
      "outputs": [],
      "source": [
        "# define the lightgbm model (from lightgbm package)\n",
        "# define the hyperparameters\n",
        "# train the model\n",
        "# try to improve the model by changing the hyperparameters on local validation (remember that using gridsearch is a bad idea, because it can't use the early stopping)\n",
        "# retrain the model on the whole train dataset\n",
        "# don't forget to specify the number of boosting rounds you found optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the catboost model (from catboost package)\n",
        "# define the hyperparameters\n",
        "# train the model\n",
        "# try to improve the model by changing the hyperparameters on local validation (remember that using gridsearch is a bad idea, because it can't use the early stopping)\n",
        "# retrain the model on the whole train dataset\n",
        "# don't forget to specify the number of boosting rounds you found optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare the results of the three models from this homework and with models from the previous homework\n",
        "# make a conclusion on which model is better and why\n",
        "# if your boosting is worse than the RF, try to improve it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load test data\n",
        "# do the same preprocessing as for train data\n",
        "\n",
        "# using retrained models make predictions on the test data for all new three models\n",
        "# save the predictions to a file\n",
        "# upload the predictions to Kaggle and make a submission\n",
        "# report the score you got and compare it with the score you got on the validation data\n",
        "# make a conclusion on how well the models generalizes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bagging homework.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
