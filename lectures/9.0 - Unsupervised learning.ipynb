{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "projector_course",
      "language": "python",
      "name": "projector_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "9.0 - Unsupervised learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enaDtmaRCz0Q"
      },
      "source": [
        "# Unsupervised learning\n",
        "\n",
        "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Instead, you need to allow the model to work on its own to discover information. It mainly deals with the unlabelled data.\n",
        "\n",
        "#### Why Unsupervised Learning?\n",
        "\n",
        "* Unsupervised machine learning finds all kind of unknown patterns in data.\n",
        "* Unsupervised methods help you to find features which can be useful for supervised learning\n",
        "* It is easier to get unlabeled data than labeled data, which needs manual relabeling.\n",
        "\n",
        "## Dimensionality Reduction\n",
        "\n",
        "### PCA (Principal Component Analysis)\n",
        "\n",
        "Principal Component Analysis is one of the easiest, most intuitive, and most frequently used methods for dimensionality reduction, projecting data onto its orthogonal feature subspace.\n",
        "\n",
        "#### When should I use PCA?\n",
        "\n",
        "1. Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\n",
        "2. Do you want to ensure your variables are independent of one another?\n",
        "3. Are you comfortable making your independent variables less interpretable?\n",
        "4. Try and check on validation (*)\n",
        "\n",
        "If you answered “yes” to all three questions, then PCA is a good method to use. If you answered “no” to question 3, you should not use PCA.\n",
        "\n",
        "#### How does PCA work\n",
        "1. Calculate the covariance matrix X of data points.\n",
        "2. Calculate eigen vectors and corresponding eigen values.\n",
        "3. Sort the eigen vectors according to their eigen values in decreasing order.\n",
        "4. Choose first k eigen vectors and that will be the new k dimensions.\n",
        "5. Transform the original n dimensional data points into k dimensions.\n",
        "\n",
        "\n",
        "#### Intuitive explanation\n",
        "1. Finding the Direction of Maximum Variance: PCA first looks for the direction in which the data varies the most. It's like finding the longest stretch in the scatterplot of your data points.\n",
        "\n",
        "2. Creating New Dimensions: Once it finds this direction (the first principal component), PCA looks for another direction that's perpendicular to the first one, where the data varies the most. This process continues until we've found all the important directions, or principal components.\n",
        "\n",
        "3. Reducing Dimensions: After finding these new dimensions, we can represent our data using fewer dimensions. We discard the dimensions that contribute the least to the overall variation in the data. This simplifies the data while retaining its important features.\n",
        "\n",
        "So, PCA helps simplify complex data by finding the most essential patterns and reducing the number of dimensions needed to represent the data, making it easier to understand and work with.\n",
        "\n",
        "You can read more detailed explanation and theory [here](https://habr.com/ru/company/ods/blog/325654/) and [here](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EhEeb8Q4BNI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bkH5NW7Cz0S"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/projector_course_data/train_titanic.csv\")\n",
        "\n",
        "X, y = data.drop(\"Survived\", axis = 1), data.Survived\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1xZkKUoCz0T"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "parameters = {\n",
        "    #default\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eta\": 0.1,\n",
        "    \"verbosity\": 0,\n",
        "    \"nthread\": 10,\n",
        "    \"random_seed\": 1,\n",
        "    \"eval_metric\": \"auc\",\n",
        "\n",
        "    # regularization parameters\n",
        "    \"max_leaves\": 32,\n",
        "    \"subsample\": 0.7,\n",
        "    \"colsample_bytree\": 0.7,\n",
        "\n",
        "    #lightgbm approach\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"grow_policy\": \"lossguide\"\n",
        "}\n",
        "\n",
        "\n",
        "xgb_train = xgb.DMatrix(X, y.values, feature_names=list(X.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YUumcIZCz0T"
      },
      "source": [
        "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
        "                 folds=skf, verbose_eval=5, early_stopping_rounds=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPfPZS-uCz0U"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K8fCxZECz0U"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0s-gVdgCz0V"
      },
      "source": [
        "for i, component in enumerate(pca.components_):\n",
        "    print(f\"{i + 1} component: {100 * pca.explained_variance_ratio_[i]:.2f}% of initial variance\")\n",
        "    print(\" + \".join(f\"{value:.2f} x {name}\"\n",
        "                     for value, name in zip(component, X.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ0OQkIjCz0V"
      },
      "source": [
        "xgb_train = xgb.DMatrix(X_reduced, y.values)\n",
        "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
        "                 folds=skf, verbose_eval=5, early_stopping_rounds=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDyIpvEICz0W"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P77uiJkSCz0W"
      },
      "source": [
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [\"Not survived\", \"Survived\"]\n",
        "colors = ['r', 'g']\n",
        "for target, color in zip([0, 1], colors):\n",
        "    indices = y == target\n",
        "    ax.scatter(X_reduced[indices, 0],\n",
        "               X_reduced[indices, 1],\n",
        "               c = color,\n",
        "               s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtp00wPgCz0X"
      },
      "source": [
        "## t-SNE\n",
        "\n",
        "It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n",
        "\n",
        "The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. Note that while the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\n",
        "\n",
        "#### Intuitive explanation\n",
        "\n",
        "1. Measuring Similarity: t-SNE starts by measuring the similarity between pairs of data points based on their original high-dimensional features. It looks at how close or similar these points are to each other in the high-dimensional space.\n",
        "\n",
        "2. Creating a Map: Then, it tries to represent these similarities in a lower-dimensional space (like a 2D or 3D plot). It does this by assigning each data point a spot on the map, trying to place similar points close together and dissimilar points farther apart.\n",
        "\n",
        "3. Iterative Optimization: The process of finding the best positions for the points on the map is iterative. t-SNE continuously adjusts the positions of points on the map to better reflect the similarities between data points in the original high-dimensional space.\n",
        "\n",
        "4. Final Visualization: Once the optimization process is done, you get a two-dimensional (or three-dimensional) map where data points that were similar in the original high-dimensional space are close together on the map, and dissimilar points are farther apart.\n",
        "\n",
        "So, t-SNE helps us visualize high-dimensional data in a way that preserves as much of the original structure and relationships between data points as possible, making it easier to understand and interpret complex datasets.\n",
        "\n",
        "The details for the underlying mathematics can be found in [paper on ArXiv](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt9RBc9bEm9v"
      },
      "source": [
        "!pip install cmake==3.18.4\n",
        "!pip install MulticoreTSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSUdynE7Cz0X"
      },
      "source": [
        "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
        "\n",
        "tsne = TSNE(n_jobs=4, )\n",
        "X_reduced_tsne = tsne.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jggGFhldCz0X"
      },
      "source": [
        "xgb_train = xgb.DMatrix(X_reduced_tsne, y.values)\n",
        "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
        "                 folds=skf, verbose_eval=5, early_stopping_rounds=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB39Iz3ACz0X"
      },
      "source": [
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.set_xlabel('Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Component 2', fontsize = 15)\n",
        "ax.set_title('2 component t-SNE', fontsize = 20)\n",
        "targets = [\"Not survived\", \"Survived\"]\n",
        "colors = ['r', 'g']\n",
        "for target, color in zip([0, 1], colors):\n",
        "    indices = y == target\n",
        "    ax.scatter(X_reduced_tsne[indices, 0],\n",
        "               X_reduced_tsne[indices, 1],\n",
        "               c = color,\n",
        "               s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoVWd48HCz0Y"
      },
      "source": [
        "## The main purpose of using dimensionality reduction is visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu_Kfy99Cz0Y"
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "tsne = TSNE()\n",
        "embeddings_tsne = tsne.fit_transform(digits.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAXBO90ECz0Y"
      },
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=digits.target,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar()\n",
        "plt.title('MNIST. t-SNE projection');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uUmUbdwCz0Y"
      },
      "source": [
        "## UMAP\n",
        "\n",
        "Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data:\n",
        "\n",
        "1. The data is uniformly distributed on a Riemannian manifold;\n",
        "2. The Riemannian metric is locally constant (or can be approximated as such);\n",
        "3. The manifold is locally connected.\n",
        "\n",
        "From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.\n",
        "\n",
        "#### Intuitive explanation\n",
        "\n",
        "1. Local Relationships: UMAP looks at how data points relate to each other locally. It focuses on understanding the relationships between neighboring points in the high-dimensional space.\n",
        "\n",
        "2. Manifold Learning: It then tries to find a low-dimensional representation of the data that preserves these local relationships as much as possible. A manifold is like a curved surface that represents the structure of the data in the high-dimensional space.\n",
        "\n",
        "3. Uniformity: UMAP aims to spread out the data points evenly in this low-dimensional space while still keeping their local relationships intact. It tries to maintain a balance between capturing the overall structure of the data and avoiding overcrowding or clustering.\n",
        "\n",
        "4. Non-linear Transformations: Unlike some other techniques, UMAP can handle non-linear relationships in the data. This means it can capture more complex patterns and structures that may not be easily represented in a linear way.\n",
        "\n",
        "5. Scalability: UMAP is also designed to be scalable, meaning it can handle large datasets efficiently without sacrificing accuracy or performance.\n",
        "\n",
        "In simple terms, UMAP helps us visualize high-dimensional data in a two-dimensional (or sometimes three-dimensional) plot while preserving the local relationships between data points as much as possible. It's useful for understanding the underlying structure of complex datasets and revealing patterns that might not be obvious in the original high-dimensional space.\n",
        "\n",
        "The details for the underlying mathematics can be found in [paper on ArXiv](https://arxiv.org/abs/1802.03426).\n",
        "\n",
        "Read about [UMAP benefits](https://github.com/lmcinnes/umap#benefits-of-umap)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b29EldolFHvN"
      },
      "source": [
        "!pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgiYxM5bCz0Y"
      },
      "source": [
        "import umap\n",
        "embedding_umap = umap.UMAP(n_neighbors=5).fit_transform(digits.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUmd9T6Cz0Z"
      },
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(embedding_umap[:, 0], embedding_umap[:, 1], c=digits.target,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar()\n",
        "plt.title('MNIST. UMAP projection');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzIakqUCz0Z"
      },
      "source": [
        "Dimensionality reduction rarely works well on the real data.\n",
        "\n",
        "T-SNE and UMAP are usually better than other dimensionality reduction algorithms.\n",
        "\n",
        "Main use cases:\n",
        "* beautiful visualizations – visualize different embeddings (texts, images, etc)\n",
        "* if you have many features (works very poorly on small dimensions)\n",
        "* estimate data complexity\n",
        "* reduce feature space (aggregate many bad features)\n",
        "* doesn't work on categorical data, on continuous numeric data – you have a chance\n",
        "* course lecture :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIOHMgalCz0Z"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n",
        "\n",
        "#### Types of Clustering\n",
        "Broadly speaking, clustering can be divided into two subgroups :\n",
        "\n",
        "* **Hard Clustering**: In hard clustering, each data point either belongs to a cluster completely or not. For example, in the above example each customer is put into one group out of the 10 groups.\n",
        "* **Soft Clustering**: In soft clustering, instead of putting each data point into a separate cluster, a probability or likelihood of that data point to be in those clusters is assigned. For example, from the above scenario each costumer is assigned a probability to be in either of 10 clusters of the retail store.\n",
        "\n",
        "![image.png](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)\n",
        "\n",
        "## K-means\n",
        "\n",
        "1. To begin, we first select a number of classes/groups to use and randomly initialize their respective center points. To figure out the number of classes to use, it’s good to take a quick look at the data and try to identify any distinct groupings. The center points are vectors of the same length as each data point vector and are the “X’s” in the graphic above.\n",
        "2. Each data point is classified by computing the distance between that point and each group center, and then classifying the point to be in the group whose center is closest to it.\n",
        "3. Based on these classified points, we recompute the group center by taking the mean of all the vectors in the group.\n",
        "4. Repeat these steps for a set number of iterations or until the group centers don’t change much between iterations. You can also opt to randomly initialize the group centers a few times, and then select the run that looks like it provided the best results.\n",
        "\n",
        "![image](https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif)\n",
        "\n",
        "A fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. **The Elbow Method** is one of the most popular methods to determine this optimal value of k.\n",
        "\n",
        "1. Distortion: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.\n",
        "2. Inertia: It is the sum of squared distances of samples to their closest cluster center."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxOBWpTqCz0Z"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnp9sTKiCz0Z"
      },
      "source": [
        "distortions = []\n",
        "inertias = []\n",
        "ks = range(1,30)\n",
        "for k in ks:\n",
        "    model = KMeans(n_clusters=k)\n",
        "    model.fit(digits.data)\n",
        "    distortions.append(sum(np.min(cdist(digits.data, model.cluster_centers_, 'euclidean'), axis=1)) / digits.data.shape[0])\n",
        "    inertias.append(model.inertia_)\n",
        "\n",
        "# Plot the elbow\n",
        "plt.plot(ks, distortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method showing the optimal k (Distortion)')\n",
        "plt.show();\n",
        "\n",
        "# Plot the elbow\n",
        "plt.plot(ks, inertias, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The Elbow Method showing the optimal k (Inertia)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtn43dOCz0a"
      },
      "source": [
        "model_kmeans = KMeans(n_clusters=10)\n",
        "model_kmeans.fit(digits.data)\n",
        "data_clusters = model_kmeans.predict(digits.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMXwCN5GCz0a"
      },
      "source": [
        "data = pd.DataFrame({\"clusters\": data_clusters, \"labels\": digits.target})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4DEDCrOCz0a"
      },
      "source": [
        "data.loc[data[\"labels\"] == 0, \"clusters\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF940wnmCz0a"
      },
      "source": [
        "data.loc[data[\"clusters\"] == 1, \"labels\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGrMuDgmCz0a"
      },
      "source": [
        "data.loc[(data[\"clusters\"] == 1) & (data[\"labels\"] == 6)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry4z915kCz0a"
      },
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(digits.data[792,:].reshape([8,8]));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLgX2ulmCz0a"
      },
      "source": [
        "## Affinity Propagation\n",
        "\n",
        "**Affinity propagation (AP)** is a centroid based clustering algorithm similar to k Means or K medoids, which does not require the estimation of the number of clusters before running the algorithm. Affinity propagation finds objects that are representative of clusters.\n",
        "\n",
        "Theoretical explanation you can read [here](https://towardsdatascience.com/math-and-intuition-behind-affinity-propagation-4ec5feae5b23)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOQuMhsGCz0a"
      },
      "source": [
        "from sklearn.cluster import AffinityPropagation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cE_5zbuCz0a"
      },
      "source": [
        "model_aff = AffinityPropagation()\n",
        "model_aff.fit(digits.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8H3OUPgCz0a"
      },
      "source": [
        "max(model_aff.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RempNdwCz0a"
      },
      "source": [
        "## Hierarchical clustering\n",
        "\n",
        "Hierarchical clustering is one of the popular and easy to understand clustering technique. This clustering technique is divided into two types:\n",
        "1. Agglomerative\n",
        "2. Divisive\n",
        "\n",
        "### Agglomerative Hierarchical clustering\n",
        "In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n",
        "\n",
        "The basic algorithm of Agglomerative is straight forward:\n",
        "1. Compute the proximity matrix\n",
        "2. Let each data point be a cluster\n",
        "3. Repeat: Merge the two closest clusters and update the proximity matrix\n",
        "4. Until only a single cluster remains\n",
        "\n",
        "Key operation is the computation of the proximity of two clusters.\n",
        "\n",
        "The Hierarchical clustering Technique can be visualized using a Dendrogram. A Dendrogram is a tree-like diagram that records the sequences of merges or splits.\n",
        "\n",
        "\n",
        "### Divisive Hierarchical clustering Technique\n",
        "In simple words, we can say that the Divisive Hierarchical clustering is exactly the opposite of the Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we consider all the data points as a single cluster and in each iteration, we separate the data points from the cluster which are not similar. Each data point which is separated is considered as an individual cluster. In the end, we’ll be left with n clusters.\n",
        "As we’re dividing the single clusters into n clusters, it is named as Divisive Hierarchical clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dIebTQwCz0b"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9uLbaeCCz0b"
      },
      "source": [
        "model_agglom = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "model_agglom.fit(digits.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrILppJpCz0b"
      },
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGg31aITCz0b"
      },
      "source": [
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model_agglom, truncate_mode='level', p=4)\n",
        "plt.xlabel(\"Number of points in node\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9WnWf4NCz0b"
      },
      "source": [
        "## Clustering metrics\n",
        "\n",
        "### Adjusted Rand Index\n",
        "\n",
        "Here, we assume that the true labels of objects are known. This metric does not depend on the labels’ values but on the data cluster split.\n",
        "\n",
        "Let N be the number of observations in a sample. Let $a$ to be the number of observation pairs with the same labels and located in the same cluster, and let $b$ to be the number of observations with different labels and located in different clusters.\n",
        "\n",
        "The Rand Index can be calculated using the following formula:\n",
        "$$RI = \\frac{2(a + b)}{n(n — 1)}$$\n",
        "In other words, it evaluates a share of observations for which these splits (initial and clustering result) are consistent. The Rand Index (RI) evaluates the similarity of the two splits of the same sample. In order for this index to be close to zero for any clustering outcomes with any n and number of clusters, it is essential to scale it, hence the Adjusted Rand Index: $$ARI = \\frac{RI — E(RI)}{max(RI) — E(RI)}$$\n",
        "This metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits. ARI takes on values in the [-1, 1] range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match ARI = 1).\n",
        "\n",
        "### Adjusted Mutual Information (AMI)\n",
        "\n",
        "This metric is similar to ARI. It is also symmetric and does not depend on the labels’ values and permutation. It is defined by the entropy function and interprets a sample split as a discrete distribution (likelihood of assigning to a cluster is equal to the percent of objects in it). The MI index is defined as the mutual information for two distributions, corresponding to the sample split into clusters. Intuitively, the mutual information measures the share of information common for both clustering splits i.e. how information about one of them decreases the uncertainty of the other one.\n",
        "Similarly to the ARI, the AMI is defined. This allows us to get rid of the MI index’s increase with the number of clusters. The AMI lies in the [0, 1] range. Values close to zero mean the splits are independent, and those close to 1 mean they are similar (with complete match at AMI = 1).\n",
        "\n",
        "\n",
        "### Silhouette\n",
        "Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified.\n",
        "\n",
        "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
        "\n",
        "The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.\n",
        "\n",
        "$$ s = \\frac{b - a}{max(a, b)} $$,\n",
        "where a – the mean distance between an object and all other data points in the same cluster; b – the smallest mean distance of an object to all points in any other cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "1. Do clustering on the digits dataset from sklearn using at least three different approaches\n",
        "2. Evaluate the quality of clustering using three metrics from the lecture\n",
        "3. Apply dimensionality reduction, then do clustering and calculate metrics.\n",
        "4. Make a comparison table of different clustering approaches with and without dimensionality reduction.\n",
        "\n",
        "In this task, we want to use 10 clusters, as we have 10 digits."
      ],
      "metadata": {
        "id": "KPHZ1MzNDo1A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0AYTQUkDrHV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}